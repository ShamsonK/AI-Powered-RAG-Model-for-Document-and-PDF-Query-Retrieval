# -*- coding: utf-8 -*-
"""AI-Powered RAG Model for Document and PDF Query Retrieval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DKSUcZnqHA5ouyLfxrIw3nC1YEPJKYAV
"""

!pip install chromadb
!pip install langchain
!pip install langchain-community
!pip install langchain-google-genai

"""**Step 1**
-Import the packages
-Croma: To store the word embeddings
-Text loader: Load the data
-RecursiveCharacterTextsplit : For the chunk the data
   - First it will chunk based on paragraph
   - then sentences and finally words
- GoogleGenerativeAIEmbedding   

"""

from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import VectorDBQA
from langchain.chains import RetrievalQA
from langchain.document_loaders import TextLoader
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings

"""**Step 2-** Document Loading

**Loading the data from Text Document**
"""

from langchain.document_loaders import TextLoader
file_path ="/content/State_union (1).txt"
loader = TextLoader(file_path)
# Load the documents
documents = loader.load()

"""**Step 3-** Splitting the Text"""

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
len(texts)

"""**Step 4-** Using Embedded Model

--Using these embedded model to retrive the text from document
"""

from langchain_google_genai import GoogleGenerativeAIEmbeddings
embeddings = GoogleGenerativeAIEmbeddings(model = 'models/embedding-001',
                                          google_api_key='AIzaSyD7kx_f_vB7xEjGSEZOIFBkutxJuXmUtKo',
                                          task_type = 'retrieval_query')

"""Step 5
- Convert the data into embedding model
"""

vectordb = Chroma.from_documents(documents=texts, embedding=embeddings)



"""**Step** **6**"""

from langchain_google_genai import ChatGoogleGenerativeAI
chat_model = ChatGoogleGenerativeAI(model = "gemini-1.5-pro",
                                    google_api_key="AIzaSyD7kx_f_vB7xEjGSEZOIFBkutxJuXmUtKo")

qa = VectorDBQA.from_chain_type(llm=chat_model,
                                 vectorstore=vectordb)

"""Asking the query from the text"""

qa.invoke("Who met the Ukrainian people")

"""**Loading data from PDF Files**"""

!pip install PyPdf

from langchain.document_loaders import PyPDFLoader
file_path ='/content/Data Science Internship Task .pdf'
loader = PyPDFLoader(file_path)
# Load the documents
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=2)
docs = text_splitter.split_documents(documents)
print(docs)
len(docs)
len(docs[0].page_content),len(docs[1].page_content)

from langchain_google_genai import GoogleGenerativeAIEmbeddings
embeddings = GoogleGenerativeAIEmbeddings(model = 'models/embedding-001',
                                          google_api_key='AIzaSyD7kx_f_vB7xEjGSEZOIFBkutxJuXmUtKo',
                                          task_type = 'retrieval_query')

vectordb = Chroma.from_documents(documents=docs, embedding=embeddings)

from langchain_google_genai import ChatGoogleGenerativeAI
chat_model = ChatGoogleGenerativeAI(model = "gemini-1.5-pro",
                                    google_api_key="AIzaSyD7kx_f_vB7xEjGSEZOIFBkutxJuXmUtKo")

qa = VectorDBQA.from_chain_type(llm=chat_model,
                                 vectorstore=vectordb)

qa.invoke("what is the internship technology")

"""**Load the Invoice Text**"""

from langchain.document_loaders import TextLoader
file_path ="/content/invoiceoutput.txt"
loader = TextLoader(file_path)
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
len(texts)

from langchain_google_genai import GoogleGenerativeAIEmbeddings
embeddings = GoogleGenerativeAIEmbeddings(model = 'models/embedding-001',
                                          google_api_key='AIzaSyD7kx_f_vB7xEjGSEZOIFBkutxJuXmUtKo',
                                          task_type = 'retrieval_query')

from langchain.vectorstores import Chroma
vectordb = Chroma.from_documents(documents=texts,embedding=embeddings)

from langchain_google_genai import ChatGoogleGenerativeAI
chat_model = ChatGoogleGenerativeAI(model = "gemini-1.5-pro",
                                    google_api_key="AIzaSyD7kx_f_vB7xEjGSEZOIFBkutxJuXmUtKo")

qa = VectorDBQA.from_chain_type(llm=chat_model,
                                 vectorstore=vectordb)

qa.invoke("what is the invoice number?")

